{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a259d3e7",
   "metadata": {},
   "source": [
    "## Turn our Fine-Tuned Model into a DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "469d86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9b57a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/Users/berkeruveyik/pythonDersleri/finetune-llm/checkpoint_models/checkpoint-426' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '/Users/berkeruveyik/pythonDersleri/finetune-llm/checkpoint_models/checkpoint-426'\n",
    "\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    dtype='auto',\n",
    "    device_map='auto',\n",
    "    attn_implementation=('eager')\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "loaded_model_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=loaded_model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667932ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of Turkey? The capital of Turkey is Ankara.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_pipeline(\"What is the capital of Turkey?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0211314b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'bug√ºn evde salata balƒ±k ve rakƒ±dan olu≈üan enfis bir yemek yedim'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_message(input):\n",
    "    return [{'role': 'user', 'content': input}]\n",
    "\n",
    "input_message = 'bug√ºn evde salata balƒ±k ve rakƒ±dan olu≈üan enfis bir yemek yedim'\n",
    "\n",
    "input_formatted = format_message(input_message)\n",
    "input_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5127ce09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'bug√ºn evde salata balƒ±k ve rakƒ±dan olu≈üan enfis bir yemek yedim.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_pipeline(input_formatted[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd860b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nbug√ºn evde salata balƒ±k ve rakƒ±dan olu≈üan enfis bir yemek yedim<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = loaded_model_pipeline.tokenizer.apply_chat_template(\n",
    "    conversation=input_formatted,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59ed31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [INFO] Input: \n",
      "<bos><start_of_turn>user\n",
      "bug√ºn evde salata balƒ±k ve rakƒ±dan olu≈üan enfis bir yemek yedim<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      " [INFO] Output: \n",
      "{'is_food_or_drink': True, 'tags': ['re', 'fi'], 'food_items': ['salata', 'balƒ±k'], 'drink_items': []}\n"
     ]
    }
   ],
   "source": [
    "loaded_model_outputs = loaded_model_pipeline(\n",
    "    text_inputs=input_prompt,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# View and compare the outputs\n",
    "print (f\" [INFO] Input: \\n{input_prompt}\\n\")\n",
    "print (f\" [INFO] Output: \\n{loaded_model_outputs[0]['generated_text'][len(input_prompt) :]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ddf3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Errors in ML are good, it shows you where to improve the model\n",
    "# There will *always* be errors in ML, since the whole science is probabilistic anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4cdc35",
   "metadata": {},
   "source": [
    "## Turn our pipeline into a demo\n",
    "\n",
    "Our demo is simple:\n",
    "* Text in, formatted LLM text out\n",
    "* Also want to time how long it takes per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2128a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"{'is_food_or_drink': True, 'tags': ['fi'], 'food_items': ['k√∂fte', 'patates'], 'drink_items': []}\",\n",
       " [{'generated_text': [{'role': 'user',\n",
       "     'content': 'bug√ºn evde k√∂fte patates yedim yanƒ±ndada kola i√ßtim √ßok g√ºzeldi'},\n",
       "    {'role': 'assistant',\n",
       "     'content': \"{'is_food_or_drink': True, 'tags': ['fi'], 'food_items': ['k√∂fte', 'patates'], 'drink_items': []}\"}]}],\n",
       " 3.4828)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "def pred_on_text(input_text):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    raw_output = loaded_model_pipeline(text_inputs=[{'role': 'user', \n",
    "                                                     'content': input_text}], \n",
    "                                       max_new_tokens=256,\n",
    "                                       disable_compile=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = round(end_time - start_time, 4)\n",
    "    \n",
    "    generate_text = raw_output[0]['generated_text'][1]['content']\n",
    "    \n",
    "    return generate_text, raw_output, total_time\n",
    "\n",
    "# pred on demo \n",
    "pred_on_text(\"bug√ºn evde k√∂fte patates yedim yanƒ±ndada kola i√ßtim √ßok g√ºzeldi\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c13fae",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5726e304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.13/site-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.13/site-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "\n",
    "def parse_generated_text(text):\n",
    "    \"\"\"Parse the generated text and format it nicely\"\"\"\n",
    "    try:\n",
    "        # Try to parse as JSON if it's JSON formatted\n",
    "        data = json.loads(text)\n",
    "        return data\n",
    "    except:\n",
    "        # If not JSON, try to extract key-value pairs\n",
    "        try:\n",
    "            # Clean and parse the text\n",
    "            text = text.strip()\n",
    "            if text.startswith('{') and text.endswith('}'):\n",
    "                data = eval(text)\n",
    "                return data\n",
    "        except:\n",
    "            pass\n",
    "    return {\"raw_output\": text}\n",
    "\n",
    "def gradio_predict(input_text):\n",
    "    \"\"\"Wrapper function for Gradio\"\"\"\n",
    "    if not input_text.strip():\n",
    "        return \"Please enter some text.\", \"0 seconds\"\n",
    "    \n",
    "    generated_text, raw_output, total_time = pred_on_text(input_text)\n",
    "    time_info = f\"{total_time} seconds\"\n",
    "    \n",
    "    # Parse the generated text\n",
    "    parsed_output = parse_generated_text(generated_text)\n",
    "    \n",
    "    # Format output as pretty JSON\n",
    "    output_json = json.dumps({\n",
    "        \"input\": input_text,\n",
    "        \"model_response\": parsed_output,\n",
    "        \"processing_time\": total_time\n",
    "    }, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return output_json, time_info\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Input Text\",\n",
    "        placeholder=\"Enter your text here...\",\n",
    "        lines=3\n",
    "    ),\n",
    "    outputs=[\n",
    "        gr.Code(label=\"Model Output (JSON)\", language=\"json\"),\n",
    "        gr.Textbox(label=\"Processing Time\")\n",
    "    ],\n",
    "    title=\"ü§ñ Fine-Tuned LLM Demo\",\n",
    "    description=\"Test your fine-tuned model. Enter text and see the model's response.\",\n",
    "    examples=[\n",
    "        [\"Today I ate meatballs and potatoes at home with cola, it was delicious\"],\n",
    "        [\"What is the capital of Turkey?\"],\n",
    "        [\"Hello, how are you?\"],\n",
    "        [\"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast and fried tomatoes\"],\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09b2af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_dict = {'np': 'nutrition_panel',\n",
    "'il': 'ingredient list',\n",
    "'me': 'menu',\n",
    "'re': 'recipe',\n",
    "'fi': 'food_items',\n",
    "'di': 'drink_items',\n",
    "'fa': 'food_advertistment',\n",
    "'fp': 'food_packaging'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab46427",
   "metadata": {},
   "source": [
    "## Upload our model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43d9877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/berkeruveyik/food-nutrition-analyzer-gemma3-270m', endpoint='https://huggingface.co', repo_type='model', repo_id='berkeruveyik/food-nutrition-analyzer-gemma3-270m')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "api = HfApi()\n",
    "repo_id = 'berkeruveyik/food-nutrition-analyzer-gemma3-270m'\n",
    "\n",
    "# Repo'yu olu≈ütur\n",
    "create_repo(repo_id=repo_id, repo_type='model', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7074e794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (7 / 7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65GB / 1.65GB, 62.1MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.61GB / 1.61GB, 62.1MB/s  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (7 / 7): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65GB / 1.65GB, 62.1MB/s  \n",
      "New Data Upload: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.61GB / 1.61GB, 62.1MB/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/berkeruveyik/food-nutrition-analyzer-gemma3-270m/commit/b26bd55aa28b8779847dca4ed3c2210ea74e3dfd', commit_message='Upload folder using huggingface_hub', commit_description='', oid='b26bd55aa28b8779847dca4ed3c2210ea74e3dfd', pr_url=None, repo_url=RepoUrl('https://huggingface.co/berkeruveyik/food-nutrition-analyzer-gemma3-270m', endpoint='https://huggingface.co', repo_type='model', repo_id='berkeruveyik/food-nutrition-analyzer-gemma3-270m'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_folder(\n",
    "    folder_path='/Users/berkeruveyik/pythonDersleri/finetune-llm/checkpoint_models/checkpoint-426',\n",
    "    repo_id=repo_id,\n",
    "    repo_type='model',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf9308",
   "metadata": {},
   "source": [
    "## Upload our demo to Hugging Face\n",
    "\n",
    "To make our demo, we need:\n",
    "* 'app.py' - Entry point for our app\n",
    "* 'README.md' - Tells people what our app does\n",
    "* requirements.txt' - Tells Hugging Face Spaces what our app requires\n",
    "    * only need: torch, transformers, gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33d56f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use mps\n",
      "/opt/anaconda3/envs/pytorch/lib/python3.13/site-packages/gradio/interface.py:171: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: theme. Please pass these parameters to launch() instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import time\n",
    "import spaces\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "MODEL_PATH = 'berkeruveyik/food-nutrition-analyzer-gemma3-270m'\n",
    "\n",
    "# Load model and tokenizer\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype='auto',\n",
    "    device_map='auto',\n",
    "    attn_implementation='eager'\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "loaded_model_pipeline = pipeline(\n",
    "    'text-generation',\n",
    "    model=loaded_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "@spaces.GPU\n",
    "def pred_on_text(input_text):\n",
    "    \"\"\"Generate prediction from input text\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    raw_output = loaded_model_pipeline(\n",
    "        text_inputs=[{'role': 'user', 'content': input_text}], \n",
    "        max_new_tokens=256\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = round(end_time - start_time, 4)\n",
    "\n",
    "    generated_text = raw_output[0]['generated_text'][1]['content']\n",
    "\n",
    "    return generated_text, raw_output, total_time\n",
    "\n",
    "def parse_generated_text(text):\n",
    "    \"\"\"Parse the generated text and format it nicely\"\"\"\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        return data\n",
    "    except:\n",
    "        try:\n",
    "            text = text.strip()\n",
    "            if text.startswith('{') and text.endswith('}'):\n",
    "                data = eval(text)\n",
    "                return data\n",
    "        except:\n",
    "            pass\n",
    "    return {\"raw_output\": text}\n",
    "\n",
    "def format_output(input_text, parsed_output, total_time):\n",
    "    \"\"\"Format output as readable text with each field on new line\"\"\"\n",
    "    output_lines = []\n",
    "    output_lines.append(f\"üìù Input: {input_text}\")\n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"‚îÅ\" * 50)\n",
    "    output_lines.append(\"\")\n",
    "    \n",
    "    if \"is_food_or_drink\" in parsed_output:\n",
    "        output_lines.append(f\"üçΩÔ∏è is_food_or_drink: {parsed_output['is_food_or_drink']}\")\n",
    "    \n",
    "    if \"tags\" in parsed_output:\n",
    "        output_lines.append(f\"üè∑Ô∏è tags: {parsed_output['tags']}\")\n",
    "    \n",
    "    if \"food_items\" in parsed_output:\n",
    "        output_lines.append(f\"üçî food_items: {parsed_output['food_items']}\")\n",
    "    \n",
    "    if \"drink_items\" in parsed_output:\n",
    "        output_lines.append(f\"ü•§ drink_items: {parsed_output['drink_items']}\")\n",
    "    \n",
    "    output_lines.append(\"\")\n",
    "    output_lines.append(\"‚îÅ\" * 50)\n",
    "    output_lines.append(f\"‚è±Ô∏è processing_time: {total_time} seconds\")\n",
    "    \n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "def gradio_predict(input_text):\n",
    "    \"\"\"Wrapper function for Gradio\"\"\"\n",
    "    if not input_text.strip():\n",
    "        return \"Please enter some text.\"\n",
    "\n",
    "    generated_text, raw_output, total_time = pred_on_text(input_text)\n",
    "    parsed_output = parse_generated_text(generated_text)\n",
    "    \n",
    "    formatted_output = format_output(input_text, parsed_output, total_time)\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_predict,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Input Text\",\n",
    "        placeholder=\"Enter your text here...\",\n",
    "        lines=3\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        label=\"Model Output\",\n",
    "        lines=12\n",
    "    ),\n",
    "    title=\"üçî Food & Nutrition Analyzer\",\n",
    "    description=\"Enter text describing food or drinks to extract structured nutrition information using a fine-tuned Gemma3 model.\",\n",
    "    examples=[\n",
    "        [\"Today I ate meatballs and potatoes at home with cola, it was delicious\"],\n",
    "        [\"British Breakfast with baked beans, fried eggs, black pudding, sausages, bacon, mushrooms, a cup of tea and toast\"],\n",
    "        [\"I had a chicken salad with olive oil dressing and sparkling water\"],\n",
    "        [\"For lunch I ordered pizza margherita with extra cheese and a glass of lemonade\"],\n",
    "        [\"Grilled salmon with steamed vegetables and white wine for dinner\"],\n",
    "        [\"My morning started with oatmeal, fresh berries, honey and a cup of black coffee\"],\n",
    "        [\"We shared nachos with guacamole, sour cream, and margaritas at the Mexican restaurant\"],\n",
    "        [\"Japanese ramen with pork belly, soft boiled egg, nori and green tea\"],\n",
    "        [\"Homemade pasta carbonara with parmesan cheese and a bottle of red wine\"],\n",
    "        [\"Smoothie bowl with banana, mango, chia seeds, granola and almond milk\"],\n",
    "    ],\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e9290c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/FoodExtractApp/README.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/FoodExtractApp/README.md\n",
    "---\n",
    "title: Food & Nutrition Analyzer\n",
    "emoji: üçî\n",
    "colorFrom: green\n",
    "colorTo: yellow\n",
    "sdk: gradio\n",
    "sdk_version: 4.0.0\n",
    "app_file: app.py\n",
    "pinned: false\n",
    "---\n",
    "\n",
    "# Food & Nutrition Analyzer üçî             \n",
    "\n",
    "A fine-tuned Gemma3-270M model for extracting structured nutrition information from food descriptions.\n",
    "\n",
    "## Description\n",
    "\n",
    "This demo uses a fine-tuned language model to analyze text descriptions of food and drinks, extracting structured information about nutrition and ingredients.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Simply enter a description of what you ate or drank, and the model will extract relevant information in a structured JSON format.\n",
    "\n",
    "## Examples\n",
    "\n",
    "- \"Today I ate meatballs and potatoes at home with cola, it was delicious\"\n",
    "- \"I had a chicken salad with olive oil dressing and sparkling water\"\n",
    "\n",
    "## Model\n",
    "\n",
    "- **Base Model**: Gemma3-270M\n",
    "- **Fine-tuned on**: Food and nutrition data\n",
    "- **Model ID**: berkeruveyik/food-nutrition-analyzer-gemma3-270m\n",
    "\n",
    "## License\n",
    "\n",
    "Please check the model license on the Hugging Face model page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ec3e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo/FoodExtractApp/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo/FoodExtractApp/requirements.txt\n",
    "transformers\n",
    "torch\n",
    "gradio\n",
    "accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9999e90f",
   "metadata": {},
   "source": [
    "### upload our demo to the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5355f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating repo on Hugging Face Hub with name: food-nutrition-analyzer\n",
      "[INFO] Full Hugging Face Hub repo name: berkeruveyik/food-nutrition-analyzer\n",
      "[INFO] Uploading demo/FoodExtractApp/ to repo: berkeruveyik/food-nutrition-analyzer\n",
      "[INFO] Upload complete! View your Space at: https://huggingface.co/spaces/berkeruveyik/food-nutrition-analyzer\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, get_full_repo_name, upload_folder\n",
    "\n",
    "# 1. Define the parameters for upload\n",
    "LOCAL_DEMO_FOLDER_PATH = \"demo/FoodExtractApp/\"\n",
    "HF_SPACE_NAME = \"food-nutrition-analyzer\"\n",
    "HF_REPO_TYPE = \"space\"\n",
    "HF_SPACE_SDK = \"gradio\"\n",
    "\n",
    "# 2. Create a Space repository on Hugging Face Hub\n",
    "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_SPACE_NAME}\")\n",
    "create_repo(\n",
    "    repo_id=HF_SPACE_NAME,\n",
    "    repo_type=HF_REPO_TYPE,\n",
    "    private=False,\n",
    "    space_sdk=HF_SPACE_SDK,\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# 3. Get the full repository name\n",
    "full_repo_name = get_full_repo_name(model_id=HF_SPACE_NAME)\n",
    "print(f\"[INFO] Full Hugging Face Hub repo name: {full_repo_name}\")\n",
    "\n",
    "# 4. Upload demo folder to Hugging Face Space\n",
    "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH} to repo: {full_repo_name}\")\n",
    "upload_folder(\n",
    "    folder_path=LOCAL_DEMO_FOLDER_PATH,\n",
    "    repo_id=full_repo_name,\n",
    "    repo_type=HF_REPO_TYPE\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Upload complete! View your Space at: https://huggingface.co/spaces/{full_repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01711016",
   "metadata": {},
   "source": [
    "## Speeding up our inference time with batched inference\n",
    "Right now our model can predict on a single sample in about 0.3 ‚Üí 1.0s.\n",
    "\n",
    "However, if we wanted to run this at scale on say 100M+ samples, this would take far too long.\n",
    "\n",
    "So we need a way to speed up our model's inference.\n",
    "\n",
    "One way to do that is batched inference.\n",
    "\n",
    "In batched inference mode, your model performs predictions on number of samples at once, this can dramatically improve sample throughput.\n",
    "\n",
    "The number of samples you can predict on at once will depend on a few factors:\n",
    "\n",
    "* The size of your model (e.g.if your model is quite large,it may only be able to predict on 1 sample at time)\n",
    "* The your compute VRAM (e.g. if your compute VRAM is already saturated, add multiple samples a time may result in errors)\n",
    "* The size of your samples lif one of your samples is 100x the size of others, this may cause errors with batched inferencel\n",
    "\n",
    "\n",
    "To find an optimal batch size for our setup, we can run an experiment:\n",
    "* Loop through different batch sizes and measure the throughput for each batch size.\n",
    "    * Why do we do this?\n",
    "        * It's hard to tell the ideal batch size ahead of time.\n",
    "        * So we experiment from say 1, 2, 4, 16, 32, 64 batch sizes and see which performs best.\n",
    "        * Just because we may get a speed up from using batch size 8, doesn't mean 64 will be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Need to turn our samples into batches\n",
    "# Step 2: Need to perform batched inference\n",
    "# Step 3: Unwind batched samples and prediction outputs to view them as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8b974574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of samples in the dataset: 1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1420/1420 [00:00<00:00, 11781.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of samples in the dataset: 1420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1420/1420 [00:00<00:00, 11781.29 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],\n",
       "        num_rows: 1136\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'image_url', 'class_label', 'source', 'char_len', 'word_count', 'syn_or_real', 'uuid', 'gpt-oss-120b-label', 'gpt-oss-120b-label-condensed', 'target_food_names_to_use', 'caption_detail_level', 'num_foods', 'target_image_point_of_view', 'messages'],\n",
       "        num_rows: 284\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mrdbourke/FoodExtract-1k\")\n",
    "\n",
    "print(f\"[INFO] Number of samples in the dataset: {len(dataset['train'])}\")\n",
    "\n",
    "def sample_to_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": sample[\"sequence\"]}, # Load the sequence from the dataset\n",
    "            {\"role\": \"system\", \"content\": sample[\"gpt-oss-120b-label-condensed\"]} # Load the gpt-oss-120b generated label\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Map our sample_to_conversation function to dataset \n",
    "dataset = dataset.map(sample_to_conversation,\n",
    "                      batched=False)\n",
    "\n",
    "# Create a train/test split\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2,\n",
    "                                            shuffle=False,\n",
    "                                            seed=42)\n",
    "\n",
    "# Number #1 rule in machine learning\n",
    "# Always train on the train set and test on the test set\n",
    "# This gives us an indication of how our model will perform in the real world\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bfc91379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Formatting test samples into list prompts...\n",
      "[INFO] Number of test sample prompts: 284\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<bos><start_of_turn>user\\nLiving Planet Goat Milk Whole Milk, 1 Litre, GMO Free, Australian Dairy, 8.75g Protein Per Serve, Good Source of Calcium.<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Need to turn our samples into batches (e.g. lists of samples)\n",
    "print(f\"[INFO] Formatting test samples into list prompts...\")\n",
    "test_input_prompts = [\n",
    "    loaded_model_pipeline.tokenizer.apply_chat_template(\n",
    "        item[\"messages\"][:1],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    for item in dataset[\"test\"]\n",
    "]\n",
    "print(f\"[INFO] Number of test sample prompts: {len(test_input_prompts)}\")\n",
    "test_input_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a84e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Need to perform batched inference and time each step\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "# Let's write a list of batch sizes to test\n",
    "chunk_sizes_to_test = [1, 4, 8, 16, 32, 64, 128]\n",
    "timing_dict = {}\n",
    "\n",
    "# Loop through each batch size and time the inference\n",
    "for CHUNK_SIZE in chunk_sizes_to_test:\n",
    "    print(f\"[INFO] Making predictions with batch size: {CHUNK_SIZE}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for chunk_number in tqdm(range(round(len(test_input_prompts) / CHUNK_SIZE))):\n",
    "        batched_inputs = test_input_prompts[(CHUNK_SIZE * chunk_number): CHUNK_SIZE * (chunk_number + 1)]\n",
    "        batched_outputs = loaded_model_pipeline(text_inputs=batched_inputs,\n",
    "                                                batch_size=CHUNK_SIZE,\n",
    "                                                max_new_tokens=256,\n",
    "                                                disable_compile=True)\n",
    "        \n",
    "        all_outputs += batched_outputs\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    timing_dict[CHUNK_SIZE] = total_time\n",
    "    print()\n",
    "    print(f\"[INFO] Total time for batch size {CHUNK_SIZE}: {total_time:.2f}s\")\n",
    "    print(\"=\"*80 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f136ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "data = timing_dict\n",
    "\n",
    "total_samples = len(dataset[\"test\"])\n",
    "\n",
    "batch_sizes = list(data.keys())\n",
    "inference_times = list(data.values())\n",
    "samples_per_second = [total_samples / time for bs, time in data.items()]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Left plot: Total Inference Time ---\n",
    "ax1.bar([str(bs) for bs in batch_sizes], inference_times, color='steelblue')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Total Inference Time (s)')\n",
    "ax1.set_title('Inference Time by Batch Size')\n",
    "\n",
    "for i, v in enumerate(inference_times):\n",
    "    ax1.text(i, v + 1, f'{v:.1f}', ha='center', fontsize=9)\n",
    "\n",
    "# --- ARROW LOGIC (Left) ---\n",
    "# 1. Identify Start (Slowest) and End (Fastest)\n",
    "start_val = max(inference_times)\n",
    "end_val = min(inference_times)\n",
    "start_idx = inference_times.index(start_val)\n",
    "end_idx = inference_times.index(end_val)\n",
    "\n",
    "speedup = start_val / end_val\n",
    "\n",
    "# 2. Draw Arrow (No Text)\n",
    "# connectionstyle \"rad=-0.3\" arcs the arrow upwards\n",
    "ax1.annotate(\"\",\n",
    "             xy=(end_idx, end_val+(0.5*end_val)),\n",
    "             xytext=(start_idx+0.25, start_val+10),\n",
    "             arrowprops=dict(arrowstyle=\"->\", color='green', lw=1.5, connectionstyle=\"arc3,rad=-0.3\"))\n",
    "\n",
    "# 3. Place Text at Midpoint\n",
    "mid_x = (start_idx + end_idx) / 2\n",
    "# Place text slightly above the highest point of the two bars\n",
    "text_y = max(start_val, end_val) + (max(inference_times) * 0.1)\n",
    "\n",
    "ax1.text(mid_x+0.5, text_y-150, f\"{speedup:.1f}x speedup\",\n",
    "         ha='center', va='bottom', fontweight='bold',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"none\", alpha=0.8))\n",
    "\n",
    "ax1.set_ylim(0, max(inference_times) * 1.35) # Increase headroom for text\n",
    "\n",
    "\n",
    "# --- Right plot: Samples per Second ---\n",
    "ax2.bar([str(bs) for bs in batch_sizes], samples_per_second, color='coral')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Samples per Second')\n",
    "ax2.set_title('Throughput by Batch Size')\n",
    "\n",
    "for i, v in enumerate(samples_per_second):\n",
    "    ax2.text(i, v + 0.05, f'{v:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# --- ARROW LOGIC (Right) ---\n",
    "# 1. Identify Start (Slowest) and End (Fastest)\n",
    "start_val_t = min(samples_per_second)\n",
    "end_val_t = max(samples_per_second)\n",
    "start_idx_t = samples_per_second.index(start_val_t)\n",
    "end_idx_t = samples_per_second.index(end_val_t)\n",
    "\n",
    "speedup_t = end_val_t / start_val_t\n",
    "\n",
    "# 2. Draw Arrow (No Text)\n",
    "ax2.annotate(\"\",\n",
    "             xy=(end_idx_t-(0.05*end_idx_t), end_val_t+(0.025*end_val_t)),\n",
    "             xytext=(start_idx_t, start_val_t+0.6),\n",
    "             arrowprops=dict(arrowstyle=\"->\", color='green', lw=1.5, connectionstyle=\"arc3,rad=-0.3\"))\n",
    "\n",
    "# 3. Place Text at Midpoint\n",
    "mid_x_t = (start_idx_t + end_idx_t) / 2\n",
    "text_y_t = max(start_val_t, end_val_t) + (max(samples_per_second) * 0.1)\n",
    "\n",
    "ax2.text(mid_x_t-0.5, text_y_t-4.5, f\"{speedup_t:.1f}x speedup\",\n",
    "         ha='center', va='bottom', fontweight='bold',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"none\", alpha=0.8))\n",
    "\n",
    "ax2.set_ylim(0, max(samples_per_second) * 1.35) # Increase headroom\n",
    "\n",
    "plt.suptitle(\"Inference with Fine-Tuned Gemma 3 270M on NVIDIA DGX Spark\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('inference_benchmark.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9960a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_second = round(len(dataset[\"test\"]) / min(timing_dict.values()), 2)\n",
    "seconds_in_a_day = 86_400\n",
    "samples_per_day = seconds_in_a_day * samples_per_second\n",
    "\n",
    "print(f\"[INFO] Number of samples per second: {samples_per_second} | Number of samples per day: {samples_per_day}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
